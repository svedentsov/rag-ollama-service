# Spring Boot RAG-сервис с Ollama

Этот проект представляет собой готовый к работе Spring Boot микросервис, демонстрирующий интеграцию с локальной LLM (через Ollama) и реализацию архитектуры **Retrieval-Augmented Generation (RAG)**.

Сервис использует векторную базу данных PostgreSQL с расширением `pgvector` для хранения и поиска документов, что позволяет LLM давать ответы, основанные на предоставленной информации, а не только на своих общих знаниях.

## Архитектура

Проект состоит из трех основных компонентов, оркестрируемых с помощью `docker-compose`:
1.  **`rag-app`**: Наше Spring Boot приложение, реализующее API и бизнес-логику.
2.  **`postgres`**: База данных PostgreSQL с расширением `pgvector`, используемая как векторное хранилище и для хранения истории чатов.
3.  **`ollama`**: Сервис для локального запуска и управления большими языковыми моделями (LLM).

## Стек технологий

- **Java 21+** и **Spring Boot 3.3.x**
- **Spring AI**: Ключевой фреймворк для интеграции с AI (Ollama, PgVector Store).
- **Ollama**: Для локального запуска LLM (например, `llama3`, `mistral`).
- **PostgreSQL + pgvector**: Для хранения векторов (эмбеддингов).
- **Spring Data JPA**: Для работы с реляционными данными (история чата).
- **Docker & Docker Compose**: Для контейнеризации и оркестрации.
- **Flyway**: Для управления миграциями схемы БД.
- **Resilience4j**: Для повышения отказоустойчивости при обращении к внешним сервисам (Ollama).
- **OpenAPI (springdoc)**: Для генерации интерактивной документации API.
- **Actuator + Prometheus**: Для мониторинга и сбора метрик.
- **Testcontainers**: Для интеграционного тестирования.

## Основной функционал

- **`POST /api/v1/documents`**: Загрузка и индексация текстовых документов. Текст разбивается на чанки, векторизуется и сохраняется в PgVector.
- **`POST /api/v1/rag/query`**: RAG-запрос. Пользовательский вопрос используется для поиска релевантных чанков в векторной БД. Найденный контекст вместе с вопросом отправляется в LLM для генерации ответа.
- **`POST /api/v1/chat`**: Прямой диалог с LLM без RAG. Сохраняет историю переписки для поддержания контекста.
- **`/swagger-ui.html`**: Интерактивная документация API (Swagger UI).
- **`/actuator/prometheus`**: Метрики для системы мониторинга Prometheus.

## Предварительные требования

1.  **Docker и Docker Compose**: [Инструкция по установке](https://docs.docker.com/get-docker/)
2.  **Java 21+ SDK**: (например, [OpenJDK](https://openjdk.java.net/))
3.  **Maven 3.8+**
4.  **Git**

## Быстрый старт

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL_РЕПОЗИТОРИЯ>
    cd rag-ollama-service
    ```

2.  **Запустите всю инфраструктуру с помощью Docker Compose:**
    Эта команда скачает образы, запустит контейнеры (PostgreSQL, Ollama) и соберет образ вашего приложения.
    ```bash
    docker-compose up --build -d
    ```
    *Флаг `--build` пересобирает образ приложения, если вы внесли изменения в код.*

3.  **Загрузите LLM-модели в Ollama:**
    Откройте новый терминал и выполните команды. Нам понадобятся две модели: `llama3` для генерации текста и `mxbai-embed-large` для создания эмбеддингов.
    ```bash
    # Загружаем модель для чата
    docker exec -it rag-ollama ollama pull llama3

    # Загружаем модель для создания эмбеддингов (векторов)
    docker exec -it rag-ollama ollama pull mxbai-embed-large
    ```
    Дождитесь окончания загрузки. Если вы хотите использовать другие модели (например, `mistral`), не забудьте изменить их имена в `application.yml`.

4.  **Запустите Spring Boot приложение (альтернативный способ без Docker):**
    Если вы не хотите запускать приложение в Docker (шаг 2), вы можете запустить его локально через Maven. Убедитесь, что PostgreSQL и Ollama запущены через `docker-compose up -d postgres ollama`.
    ```bash
    mvn spring-boot:run
    ```
    Сервис будет доступен на `http://localhost:8080`.

## Использование API

После запуска приложения самый простой способ взаимодействовать с API — через Swagger UI.

1.  **Откройте Swagger UI в браузере:**
    [http://localhost:8080/swagger-ui.html](http://localhost:8080/swagger-ui.html)

2.  **Загрузите документ для RAG:**
    - Найдите эндпоинт `POST /api/v1/documents`.
    - Нажмите "Try it out".
    - Вставьте в тело запроса JSON с вашим текстом. Например:
      ```json
      {
        "sourceName": "spring-ai-doc.txt",
        "text": "Spring AI - это проект, целью которого является упрощение разработки приложений, использующих искусственный интеллект. Он предоставляет абстракции для взаимодействия с различными AI моделями, включая чаты и модели для создания эмбеддингов. Проект легко интегрируется со Spring Boot и экосистемой Spring."
      }
      ```
    - Нажмите "Execute". Документ будет обработан и сохранен в векторной базе данных.

3.  **Задайте вопрос с использованием RAG:**
    - Найдите эндпоинт `POST /api/v1/rag/query`.
    - Нажмите "Try it out".
    - Введите ваш вопрос, основанный на загруженном документе:
      ```json
      {
        "query": "Что такое Spring AI?",
        "topK": 3,
        "similarityThreshold": 0.7
      }
      ```
    - Нажмите "Execute". Вы получите ответ, сгенерированный LLM на основе контекста из вашего документа.
