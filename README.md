# Spring Boot RAG-сервис с Ollama

Этот проект представляет собой готовый к работе Spring Boot микросервис, демонстрирующий интеграцию с локальной LLM (через Ollama) и реализацию архитектуры **Retrieval-Augmented Generation (RAG)**.

Сервис использует векторную базу данных PostgreSQL с расширением `pgvector` для хранения и поиска документов, что позволяет LLM давать ответы, основанные на предоставленной информации, а не на своих общих знаниях.

## Стек технологий

- **Java 21+**
- **Spring Boot 3.3.x**
- **Spring AI** (Ollama, PgVector Store)
- **Ollama**: для локального запуска LLM (например, `llama3`, `mistral`)
- **PostgreSQL + pgvector**: для хранения векторов
- **Spring Data JPA**: для работы с реляционными данными (история чата)
- **Maven**: система сборки
- **Docker Compose**: для оркестрации инфраструктуры
- **Flyway**: для миграций схемы БД
- **Testcontainers**: для интеграционного тестирования
- **OpenAPI (Swagger)**: для документации API
- **Spring Boot Actuator + Prometheus**: для мониторинга

## Функционал

- `POST /api/v1/documents`: Загрузка и индексация текстовых документов. Текст разбивается на чанки, векторизуется и сохраняется в PgVector.
- `POST /api/v1/rag/query`: RAG-запрос. Пользовательский вопрос используется для поиска релевантных чанков в векторной БД. Найденный контекст вместе с вопросом отправляется в LLM для генерации ответа.
- `POST /api/v1/chat`: Прямой диалог с LLM без RAG. Сохраняет историю переписки.
- `GET /actuator/health`: Проверка состояния сервиса.
- `GET /actuator/prometheus`: Метрики для Prometheus.
- `GET /swagger-ui.html`: Интерактивная документация API.

## Предварительные требования

1.  **Docker и Docker Compose**: [Инструкция по установке](https://docs.docker.com/get-docker/)
2.  **Java 21+ SDK**: (например, [OpenJDK](https://openjdk.java.net/))
3.  **Maven 3.8+**
4.  **Git**

## Быстрый старт

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL_РЕПОЗИТОРИЯ>
    cd rag-ollama-service
    ```

2.  **Запустите инфраструктуру (PostgreSQL и Ollama):**
    Эта команда скачает образы и запустит контейнеры в фоновом режиме.
    ```bash
    docker compose up -d
    ```

3.  **Загрузите модели в Ollama:**
    Откройте новый терминал и выполните команды внутри запущенного контейнера `ollama`. Нам понадобятся две модели: `llama3` для чата и `mxbai-embed-large` для создания эмбеддингов.
    ```bash
    # Загружаем модель для чата
    docker exec -it rag-ollama ollama pull llama3

    # Загружаем модель для создания эмбеддингов (векторов)
    docker exec -it rag-ollama ollama pull mxbai-embed-large
    ```
    Дождитесь окончания загрузки моделей. Вы можете использовать и другие модели, например `mistral`, но не забудьте изменить `spring.ai.ollama.chat.options.model` в `application.yml`.

4.  **Запустите Spring Boot приложение:**
    ```bash
    mvn spring-boot:run
    ```
    Сервис будет запущен на порту `8080`.

## Использование API

После запуска вы можете взаимодействовать с API. Проще всего это делать через Swagger UI.

### 1. Откройте Swagger UI

Перейдите в браузере по адресу: [http://localhost:8080/swagger-ui.html](http://localhost:8080/swagger-ui.html)

### 2. Загрузите документ

- Найдите эндпоинт `POST /api/v1/documents`.
- Нажмите "Try it out".
- Вставьте в тело запроса JSON с вашим текстом. Например:
  ```json
  {
    "sourceName": "spring-ai-doc.txt",
    "text": "Spring AI - это проект, целью которого является упрощение разработки приложений, использующих искусственный интеллект. Он предоставляет абстракции для взаимодействия с различными AI моделями, включая чаты и модели для создания эмбеддингов. Проект легко интегрируется со Spring Boot и экосистемой Spring."
  }
